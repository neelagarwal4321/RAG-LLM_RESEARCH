{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKshUw8kZo4BOuOQrcT7FB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelagarwal4321/RAG-LLM_RESEARCH/blob/main/logistic_practice_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Classification"
      ],
      "metadata": {
        "id": "DMC19-od_mJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = list(set(y))\n",
        "        self.params = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = [x for x,label in zip(X,y) if label==c]\n",
        "            self.params[c] = {\n",
        "                \"mean\": [sum(f)/len(f) for f in zip(*X_c)],\n",
        "                \"var\": [sum((xi-mu)**2 for xi in f)/len(f) for f,mu in zip(zip(*X_c), [sum(f)/len(f) for f in zip(*X_c)])],\n",
        "                \"prior\": len(X_c)/len(X)\n",
        "            }\n",
        "\n",
        "    def gaussian(self, x, mean, var):\n",
        "        return (1/math.sqrt(2*math.pi*var)) * math.exp(-(x-mean)**2/(2*var))\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        posteriors = {}\n",
        "        for c in self.classes:\n",
        "            prior = math.log(self.params[c][\"prior\"])\n",
        "            cond = sum(math.log(self.gaussian(xi, mu, var)) for xi,mu,var in zip(x,self.params[c][\"mean\"],self.params[c][\"var\"]))\n",
        "            posteriors[c] = prior + cond\n",
        "        return max(posteriors, key=posteriors.get)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self.predict_one(x) for x in X]\n"
      ],
      "metadata": {
        "id": "xucmS1QxAItq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Logistic Regression"
      ],
      "metadata": {
        "id": "OgKCWsai_qzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class BayesianLogisticRegression:\n",
        "    def __init__(self, lr=0.01, epochs=1000, lam=1.0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.lam = lam  # regularization strength (Bayesian prior)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1/(1+math.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n, m = len(X), len(X[0])\n",
        "        self.w = [random.random() for _ in range(m)]\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n):\n",
        "                z = sum(self.w[j]*X[i][j] for j in range(m)) + self.b\n",
        "                p = self.sigmoid(z)\n",
        "                error = y[i] - p\n",
        "\n",
        "                # Update with prior penalty\n",
        "                for j in range(m):\n",
        "                    self.w[j] += self.lr * (error*X[i][j] - self.lam*self.w[j])\n",
        "                self.b += self.lr * error\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        z = sum(self.w[j]*x[j] for j in range(len(x))) + self.b\n",
        "        return 1 if self.sigmoid(z) >= 0.5 else 0\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self.predict_one(x) for x in X]\n"
      ],
      "metadata": {
        "id": "C7S6HjzKAM-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine"
      ],
      "metadata": {
        "id": "mDpAeeSA_tW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SVM:\n",
        "    def __init__(self, lr=0.01, epochs=1000, lam=0.01):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.lam = lam\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n, m = len(X), len(X[0])\n",
        "        self.w = [0.0 for _ in range(m)]\n",
        "        self.b = 0\n",
        "\n",
        "        # Convert labels {0,1} â†’ {-1,1}\n",
        "        y_ = [1 if label==1 else -1 for label in y]\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n):\n",
        "                condition = y_[i]*(sum(self.w[j]*X[i][j] for j in range(m)) + self.b)\n",
        "                if condition >= 1:\n",
        "                    for j in range(m):\n",
        "                        self.w[j] -= self.lr * (2*self.lam*self.w[j])\n",
        "                else:\n",
        "                    for j in range(m):\n",
        "                        self.w[j] += self.lr * (y_[i]*X[i][j] - 2*self.lam*self.w[j])\n",
        "                    self.b += self.lr * y_[i]\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        return 1 if sum(self.w[j]*x[j] for j in range(len(x))) + self.b >= 0 else 0\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self.predict_one(x) for x in X]"
      ],
      "metadata": {
        "id": "M30PLFay_2nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X = [row[:2] for row in dataset]\n",
        "y = [row[2] for row in dataset]\n",
        "\n",
        "# Naive Bayes\n",
        "nb = NaiveBayes()\n",
        "nb.fit(X,y)\n",
        "print(\"Naive Bayes predictions:\", nb.predict(X))\n",
        "\n",
        "# Bayesian Logistic Regression\n",
        "blr = BayesianLogisticRegression(lr=0.01, epochs=2000, lam=0.1)\n",
        "blr.fit(X,y)\n",
        "print(\"Bayesian Logistic Regression predictions:\", blr.predict(X))\n",
        "\n",
        "# SVM\n",
        "svm = SVM(lr=0.01, epochs=2000, lam=0.01)\n",
        "svm.fit(X,y)\n",
        "print(\"SVM predictions:\", svm.predict(X))"
      ],
      "metadata": {
        "id": "pDWijqRxAfpW",
        "outputId": "a2ff667f-5281-4cfa-f835-308ef201b867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes predictions: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "Bayesian Logistic Regression predictions: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "SVM predictions: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vehKNjBAhX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}